#!/usr/bin/env python3
"""
eval_runner.py â€” Compare LLM-only vs RAG on the gold questions.
Generates eval_results.csv and contributes to final_report.md.
"""

import json
import csv
import os
import sys
import time

sys.path.insert(0, os.path.join(os.path.dirname(__file__), ".."))

from rag.rag_pipeline import ask, ask_llm_only
from rag.ollama_client import generate

EVAL_DIR = os.path.dirname(__file__)
GOLD_PATH = os.path.join(EVAL_DIR, "gold_questions.json")
RESULTS_DIR = os.path.join(os.path.dirname(__file__), "..", "results")
CONFIG_PATH = os.path.join(os.path.dirname(__file__), "..", "config.json")


def load_config():
    with open(CONFIG_PATH, "r", encoding="utf-8") as f:
        return json.load(f)


def load_gold():
    with open(GOLD_PATH, "r", encoding="utf-8") as f:
        return json.load(f)


def auto_eval_score(expected: str, actual: str) -> dict:
    """
    Heuristic scoring of answer quality by comparing with expected answer.
    """
    if not actual or actual.startswith("ERROR"):
        return {"accuracy_score": 1, "faithfulness_score": 1, "hallucination_flag": True}

    expected_words = set(expected.split())
    actual_words = set(actual.split())
    overlap = expected_words & actual_words
    overlap_ratio = len(overlap) / max(len(expected_words), 1)

    # Accuracy: how much of the expected answer is covered
    if overlap_ratio > 0.5:
        accuracy = 5
    elif overlap_ratio > 0.3:
        accuracy = 4
    elif overlap_ratio > 0.15:
        accuracy = 3
    elif overlap_ratio > 0.05:
        accuracy = 2
    else:
        accuracy = 1

    # Faithfulness: shorter, focused answers score higher
    word_count = len(actual.split())
    if word_count < 200 and overlap_ratio > 0.3:
        faithfulness = 5
    elif word_count < 300:
        faithfulness = 4
    elif word_count < 500:
        faithfulness = 3
    else:
        faithfulness = 2

    # Hallucination: flag if answer is very long but low overlap
    hallucination = word_count > 200 and overlap_ratio < 0.1

    return {
        "accuracy_score": accuracy,
        "faithfulness_score": faithfulness,
        "hallucination_flag": hallucination,
    }


def run_evaluation():
    cfg = load_config()
    model = cfg.get("chosen_model") or cfg["models"][0]
    gold = load_gold()
    os.makedirs(RESULTS_DIR, exist_ok=True)

    csv_path = os.path.join(RESULTS_DIR, "eval_results.csv")
    fieldnames = [
        "question_id", "mode", "question", "expected_answer",
        "answer", "accuracy_score", "faithfulness_score",
        "hallucination_flag", "latency",
    ]

    rows = []
    llm_scores = []
    rag_scores = []

    print(f"ğŸ”¬ Running evaluation with model: {model}")
    print(f"   Questions: {len(gold)}")
    print()

    for q in gold:
        qid = q["question_id"]
        question = q["question"]
        expected = q["expected_answer_short"]

        # â”€â”€â”€ LLM Only â”€â”€â”€
        print(f"  Q{qid} [LLM]...", end=" ", flush=True)
        try:
            llm_result = ask_llm_only(question, model=model)
            llm_answer = llm_result["answer"]
            llm_latency = llm_result["generation_time"]
        except Exception as e:
            llm_answer = f"ERROR: {e}"
            llm_latency = -1

        llm_eval = auto_eval_score(expected, llm_answer)
        llm_scores.append(llm_eval)
        rows.append({
            "question_id": qid, "mode": "LLM",
            "question": question, "expected_answer": expected,
            "answer": llm_answer,
            "accuracy_score": llm_eval["accuracy_score"],
            "faithfulness_score": llm_eval["faithfulness_score"],
            "hallucination_flag": llm_eval["hallucination_flag"],
            "latency": llm_latency,
        })
        print(f"done ({llm_latency}s)")

        # â”€â”€â”€ RAG â”€â”€â”€
        print(f"  Q{qid} [RAG]...", end=" ", flush=True)
        try:
            rag_result = ask(question, model=model, top_k=5)
            rag_answer = rag_result["answer"]
            rag_latency = rag_result["retrieval_time"] + rag_result["generation_time"]
        except Exception as e:
            rag_answer = f"ERROR: {e}"
            rag_latency = -1

        rag_eval = auto_eval_score(expected, rag_answer)
        rag_scores.append(rag_eval)
        rows.append({
            "question_id": qid, "mode": "RAG",
            "question": question, "expected_answer": expected,
            "answer": rag_answer,
            "accuracy_score": rag_eval["accuracy_score"],
            "faithfulness_score": rag_eval["faithfulness_score"],
            "hallucination_flag": rag_eval["hallucination_flag"],
            "latency": rag_latency,
        })
        print(f"done ({rag_latency}s)")

    # Write CSV
    with open(csv_path, "w", newline="", encoding="utf-8") as f:
        writer = csv.DictWriter(f, fieldnames=fieldnames)
        writer.writeheader()
        writer.writerows(rows)
    print(f"\nâœ… Eval results saved to {csv_path}")

    # Write final report
    write_final_report(model, gold, llm_scores, rag_scores, rows)


def write_final_report(model, gold, llm_scores, rag_scores, rows):
    """Generate the final comparison report."""
    report_path = os.path.join(RESULTS_DIR, "final_report.md")

    def avg(scores, key):
        vals = [s[key] for s in scores if isinstance(s[key], (int, float))]
        return round(sum(vals) / max(len(vals), 1), 2)

    def count_flag(scores):
        return sum(1 for s in scores if s.get("hallucination_flag"))

    llm_rows = [r for r in rows if r["mode"] == "LLM" and r["latency"] > 0]
    rag_rows = [r for r in rows if r["mode"] == "RAG" and r["latency"] > 0]
    llm_avg_lat = round(sum(r["latency"] for r in llm_rows) / max(len(llm_rows), 1), 2)
    rag_avg_lat = round(sum(r["latency"] for r in rag_rows) / max(len(rag_rows), 1), 2)

    with open(report_path, "w", encoding="utf-8") as f:
        f.write("# ØªÙ‚Ø±ÙŠØ± Ø§Ù„ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ â€” Ù†Ø¸Ø§Ù… RAG Ù„ÙƒØªØ§Ø¨ Ø£Ù†ØªÙŠØ®Ø±ÙŠØ³ØªÙˆØ³\n\n")
        f.write(f"**Ø§Ù„ØªØ§Ø±ÙŠØ®:** {time.strftime('%Y-%m-%d %H:%M')}\n\n")
        f.write(f"**Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…:** `{model}`\n\n")
        f.write(f"**Ø¹Ø¯Ø¯ Ø§Ù„Ø£Ø³Ø¦Ù„Ø©:** {len(gold)}\n\n")

        f.write("---\n\n")

        f.write("## 1. Ù…Ù†Ù‡Ø¬ÙŠØ© Ø§Ù„Ø§Ø³ØªØ¨ÙŠØ§Ù†\n\n")
        f.write("- ØªÙ… Ø§Ø®ØªØ¨Ø§Ø± 3 Ù†Ù…Ø§Ø°Ø¬ Ù…Ø­Ù„ÙŠØ© Ø¹Ø¨Ø± Ollama (ÙˆØ¶Ø¹ CPU ÙÙ‚Ø·)\n")
        f.write("- 30 Ø³Ø¤Ø§Ù„Ø§Ù‹ Ø¹Ø±Ø¨ÙŠØ§Ù‹ ÙÙŠ 5 ÙØ¦Ø§Øª: ÙƒØªØ§Ø¨Ø© ÙØµØ­Ù‰ØŒ ØªÙ„Ø®ÙŠØµØŒ ÙÙ‡Ù… Ù‚Ø±Ø§Ø¦ÙŠØŒ ØªÙÙƒÙŠØ± Ù…Ù†Ø·Ù‚ÙŠØŒ Ù„Ù‡Ø¬Ø§Øª\n")
        f.write("- Ø§Ù„ØªÙ‚ÙŠÙŠÙ…: ØªÙ„Ù‚Ø§Ø¦ÙŠ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ù†Ø³Ø¨Ø© Ø§Ù„Ù†Øµ Ø§Ù„Ø¹Ø±Ø¨ÙŠØŒ Ø§Ù„Ø·ÙˆÙ„ØŒ Ø§Ù„ØªØ±ÙƒÙŠØ¨\n\n")

        f.write("## 2. ØªØ±ØªÙŠØ¨ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬\n\n")
        f.write("Ø±Ø§Ø¬Ø¹ `results/survey_summary.md` Ù„Ù„ØªØ±ØªÙŠØ¨ Ø§Ù„ØªÙØµÙŠÙ„ÙŠ.\n\n")

        f.write(f"## 3. ØªØ¨Ø±ÙŠØ± Ø§Ø®ØªÙŠØ§Ø± Ø§Ù„Ù†Ù…ÙˆØ°Ø¬\n\n")
        f.write(f"ØªÙ… Ø§Ø®ØªÙŠØ§Ø± `{model}` Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø£Ø¹Ù„Ù‰ Ù…ØªÙˆØ³Ø· Ø¯Ø±Ø¬Ø§Øª Ø´Ø§Ù…Ù„ ÙÙŠ Ø§Ù„Ø§Ø³ØªØ¨ÙŠØ§Ù† Ø§Ù„Ø¹Ø±Ø¨ÙŠ.\n\n")

        f.write("## 4. Ù‡Ù†Ø¯Ø³Ø© Ù†Ø¸Ø§Ù… RAG\n\n")
        f.write("```\n")
        f.write("Ø³Ø¤Ø§Ù„ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…\n")
        f.write("    â†“\n")
        f.write("ØªØ¶Ù…ÙŠÙ† Ø§Ù„Ø³Ø¤Ø§Ù„ (multilingual-e5-small)\n")
        f.write("    â†“\n")
        f.write("Ø§Ø³ØªØ±Ø¬Ø§Ø¹ Ø£Ù‚Ø±Ø¨ 5 Ù…Ù‚Ø§Ø·Ø¹ Ù…Ù† FAISS\n")
        f.write("    â†“\n")
        f.write("Ø¨Ù†Ø§Ø¡ Ø³Ø¤Ø§Ù„ Ø¹Ø±Ø¨ÙŠ Ù…Ø¹ Ø§Ù„Ø³ÙŠØ§Ù‚\n")
        f.write("    â†“\n")
        f.write("ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ø¨Ø± Ollama\n")
        f.write("    â†“\n")
        f.write("Ø¹Ø±Ø¶ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© + Ø§Ù„Ù…ØµØ§Ø¯Ø± + Ø²Ù…Ù† Ø§Ù„Ø§Ø³ØªØ¬Ø§Ø¨Ø©\n")
        f.write("```\n\n")

        f.write("## 5. Ù…Ù‚Ø§Ø±Ù†Ø© LLM Ù…Ù‚Ø§Ø¨Ù„ RAG\n\n")
        f.write("| Ø§Ù„Ù…Ù‚ÙŠØ§Ø³ | LLM ÙÙ‚Ø· | RAG |\n")
        f.write("|---------|---------|-----|\n")
        f.write(f"| Ø§Ù„Ø¯Ù‚Ø© (1-5) | {avg(llm_scores, 'accuracy_score')} | {avg(rag_scores, 'accuracy_score')} |\n")
        f.write(f"| Ø§Ù„Ø£Ù…Ø§Ù†Ø© (1-5) | {avg(llm_scores, 'faithfulness_score')} | {avg(rag_scores, 'faithfulness_score')} |\n")
        f.write(f"| Ø­Ø§Ù„Ø§Øª Ø§Ù„Ù‡Ù„ÙˆØ³Ø© | {count_flag(llm_scores)}/{len(llm_scores)} | {count_flag(rag_scores)}/{len(rag_scores)} |\n")
        f.write(f"| Ù…ØªÙˆØ³Ø· Ø²Ù…Ù† Ø§Ù„Ø§Ø³ØªØ¬Ø§Ø¨Ø© | {llm_avg_lat}Ø« | {rag_avg_lat}Ø« |\n\n")

        f.write("## 6. Ø§Ù„Ù…Ù„Ø§Ø­Ø¸Ø§Øª\n\n")

        # Dynamic observations
        acc_diff = avg(rag_scores, 'accuracy_score') - avg(llm_scores, 'accuracy_score')
        if acc_diff > 0:
            f.write(f"- Ù†Ø¸Ø§Ù… RAG Ø­Ù‚Ù‚ Ø¯Ù‚Ø© Ø£Ø¹Ù„Ù‰ Ø¨ÙØ§Ø±Ù‚ {acc_diff} Ù†Ù‚Ø·Ø© Ù…Ù‚Ø§Ø±Ù†Ø© Ø¨Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙˆØ­Ø¯Ù‡\n")
        elif acc_diff < 0:
            f.write(f"- Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙˆØ­Ø¯Ù‡ Ø­Ù‚Ù‚ Ø¯Ù‚Ø© Ø£Ø¹Ù„Ù‰ Ø¨ÙØ§Ø±Ù‚ {abs(acc_diff)} Ù†Ù‚Ø·Ø© â€” Ù‚Ø¯ ÙŠÙƒÙˆÙ† Ø§Ù„Ø³Ø¨Ø¨ Ø¬ÙˆØ¯Ø© Ø§Ù„Ø§Ø³ØªØ±Ø¬Ø§Ø¹\n")
        else:
            f.write("- Ø§Ù„Ø¯Ù‚Ø© Ù…ØªØ³Ø§ÙˆÙŠØ© Ø¨ÙŠÙ† Ø§Ù„ÙˆØ¶Ø¹ÙŠÙ†\n")

        llm_hall = count_flag(llm_scores)
        rag_hall = count_flag(rag_scores)
        if rag_hall < llm_hall:
            f.write(f"- RAG Ù‚Ù„Ù‘Ù„ Ù…Ù† Ø­Ø§Ù„Ø§Øª Ø§Ù„Ù‡Ù„ÙˆØ³Ø© ({rag_hall} Ù…Ù‚Ø§Ø¨Ù„ {llm_hall})\n")

        f.write("- Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ ØªØ¹Ù…Ù„ ÙÙŠ ÙˆØ¶Ø¹ CPU â€” Ø§Ù„Ø£Ø¯Ø§Ø¡ Ø³ÙŠØªØ­Ø³Ù† ÙƒØ«ÙŠØ±Ø§Ù‹ Ù…Ø¹ GPU\n")
        f.write("- Ø§Ù„ØªÙ‚ÙŠÙŠÙ… Ø§Ù„ØªÙ„Ù‚Ø§Ø¦ÙŠ ØªÙ‚Ø±ÙŠØ¨ÙŠ â€” Ø§Ù„ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ø¨Ø´Ø±ÙŠ Ø³ÙŠÙƒÙˆÙ† Ø£ÙƒØ«Ø± Ø¯Ù‚Ø©\n\n")

        f.write("## 7. Ø§Ù„Ù‚ÙŠÙˆØ¯\n\n")
        f.write("- ØªØ¹Ù…Ù„ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø¹Ù„Ù‰ CPU ÙÙ‚Ø· (Ø¨Ø·Ø¡ Ù…Ù„Ø­ÙˆØ¸)\n")
        f.write("- Ø§Ù„ØªÙ‚ÙŠÙŠÙ… Ù‡ÙŠÙˆØ±ÙŠØ³ØªÙŠÙƒÙŠ ÙˆÙ„ÙŠØ³ Ø¨Ø´Ø±ÙŠØ§Ù‹\n")
        f.write("- Ø­Ø¬Ù… Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ù…Ø­Ø¯ÙˆØ¯ (3B-7B) Ø¨Ø³Ø¨Ø¨ Ù‚ÙŠÙˆØ¯ Ø§Ù„Ù…Ø³Ø§Ø­Ø©\n")
        f.write("- Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ØªØ¶Ù…ÙŠÙ† ØµØºÙŠØ± â€” Ù†Ù…ÙˆØ°Ø¬ Ø£ÙƒØ¨Ø± Ù‚Ø¯ ÙŠØ­Ø³Ù† Ø§Ù„Ø§Ø³ØªØ±Ø¬Ø§Ø¹\n\n")

        f.write("## 8. Ø§Ù„Ø®Ù„Ø§ØµØ©\n\n")
        f.write(f"ØªÙ… Ø¨Ù†Ø¬Ø§Ø­ Ø¨Ù†Ø§Ø¡ Ù†Ø¸Ø§Ù… RAG ÙƒØ§Ù…Ù„ Ù„ÙƒØªØ§Ø¨ Ø£Ù†ØªÙŠØ®Ø±ÙŠØ³ØªÙˆØ³ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù†Ù…ÙˆØ°Ø¬ `{model}` Ù…Ø­Ù„ÙŠØ§Ù‹. ")
        f.write("Ø§Ù„Ù†Ø¸Ø§Ù… ÙŠØ´Ù…Ù„ ÙˆØ§Ø¬Ù‡Ø© Ù…Ø­Ø§Ø¯Ø«Ø© ØªÙØ§Ø¹Ù„ÙŠØ©ØŒ Ø§Ø³ØªØ±Ø¬Ø§Ø¹ Ø¯Ù„Ø§Ù„ÙŠ Ù…Ù† Ø§Ù„Ù†ØµØŒ ÙˆÙ…Ù‚Ø§Ø±Ù†Ø© Ø´Ø§Ù…Ù„Ø© Ø¨ÙŠÙ† ")
        f.write("Ø£Ø¯Ø§Ø¡ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ù…ÙØ±Ø¯Ù‡ ÙˆØ£Ø¯Ø§Ø¦Ù‡ Ù…Ø¹ Ù†Ø¸Ø§Ù… RAG.\n")

    print(f"âœ… Final report saved to {report_path}")


if __name__ == "__main__":
    run_evaluation()
